{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase Classification by Fine-tuning BERT\n",
    "\n",
    "Pre-trained language\n",
    "representations have been shown to improve many downstream NLP tasks such as\n",
    "question answering, and natural language inference. To apply pre-trained\n",
    "representations to these tasks, there are two strategies:\n",
    "\n",
    "1. **feature-based**\n",
    "approach, which uses the pre-trained representations as additional\n",
    "features to\n",
    "the downstream task.\n",
    "2. **fine-tuning** based approach, which trains the\n",
    "downstream tasks by\n",
    "fine-tuning pre-trained parameters.\n",
    "\n",
    "While feature-based\n",
    "approaches such as ELMo [3] (introduced in the previous tutorial) are effective\n",
    "in improving many downstream tasks, they require task-specific architectures.\n",
    "Devlin, Jacob, et al proposed BERT [1] (Bidirectional Encoder Representations\n",
    "from Transformers), which **fine-tunes** deep bidirectional representations on a\n",
    "wide range of tasks with minimal task-specific parameters, and obtained state-\n",
    "of-the-art results.\n",
    "\n",
    "In this tutorial, we will focus on fine-tuning with the\n",
    "pre-trained BERT model to classify semantically equivalent sentence pairs.\n",
    "Specifically, we will:\n",
    "\n",
    "1. load the state-of-the-art pre-trained BERT model and\n",
    "attach an additional layer for classification,\n",
    "2. process and transform sentence\n",
    "pair data for the task at hand, and\n",
    "3. fine-tune BERT model for sentence\n",
    "classification.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "To run this tutorial locally, please:\n",
    "\n",
    "- [install gluonnlp](http://gluon-nlp.mxnet.io/#installation), and\n",
    "- click the\n",
    "download button at the top of the tutorial page to get all related code.\n",
    "\n",
    "Then\n",
    "we start with some usual preparation such as importing libraries\n",
    "and setting the\n",
    "environment.\n",
    "\n",
    "### Load MXNet and GluonNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "from bert import data, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "# change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Pre-trained BERT Model\n",
    "\n",
    "The list of pre-trained BERT model available\n",
    "in GluonNLP can be found\n",
    "[here](../../model_zoo/bert/index.rst).\n",
    "\n",
    "In this\n",
    "tutorial, we will load the BERT\n",
    "BASE model trained on uncased book corpus and\n",
    "English Wikipedia dataset in\n",
    "GluonNLP model zoo.\n",
    "\n",
    "### Get BERT\n",
    "\n",
    "Let's first take\n",
    "a look at the BERT model\n",
    "architecture for sentence pair classification below:\n",
    "<div style=\"width:\n",
    "500px;\">![bert-sentence-pair](bert-sentence-pair.png)</div>\n",
    "where the model takes a pair of\n",
    "sequences and *pools* the representation of the\n",
    "first token in the sequence.\n",
    "Note that the original BERT model was trained for\n",
    "masked language model and next\