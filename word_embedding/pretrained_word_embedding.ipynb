{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Word Embeddings\n",
    "\n",
    "In this notebook, we'll demonstrate how to use pre-trained word embeddings.\n",
    "To see why word embeddings are useful, it's worth comparing them to the alternative.\n",
    "Without word embeddings, we might represent each word by a one-hot vector `[0, ...,0, 1, 0, ... 0]`,\n",
    "that takes value `1` at the index corresponding to the appropriate vocabulary word,\n",
    "and value `0` everywhere else.\n",
    "The weight matrices connecting our word-level inputs to the network's hidden layers would each be $v \\times h$,\n",
    "where $v$ is the size of the vocabulary and $h$ is the size of the hidden layer.\n",
    "With 100,000 words feeding into an LSTM layer with $1000$ nodes, we would need to learn\n",
    "$4$ different weight matrices (one for each of the LSTM gates), each with 100M weights, and thus 400 million parameters in total.\n",
    "\n",
    "Fortunately, it turns out that a number of efficient techniques\n",
    "can quickly discover broadly useful word embeddings in an *unsupervised* manner.\n",
    "These embeddings map each word onto a low-dimensional vector $w \\in R^d$ with $d$ commonly chosen to be roughly $100$.\n",
    "Intuitively, these embeddings are chosen based on the contexts in which words appear.\n",
    "Words that appear in similar contexts (like \"tennis\" and \"racquet\") should have similar embeddings\n",
    "while words that do not like (like \"rat\" and \"gourmet\") should have dissimilar embeddings.\n",
    "\n",
    "Practitioners of deep learning for NLP typically initialize their models\n",
    "using *pretrained* word embeddings, bringing in outside information, and reducing the number of parameters that a neural network needs to learn from scratch.\n",
    "\n",
    "\n",
    "Two popular word embeddings are GloVe and fastText.\n",
    "The following examples uses pre-trained word embeddings drawn from the following sources:\n",
    "\n",
    "* GloVe project website：https://nlp.stanford.edu/projects/glove/\n",
    "* fastText project website：https://fasttext.cc/\n",
    "\n",
    "To begin, let's first import a few packages that we'll need for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "import gluonnlp as nlp\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vocabulary with Word Embeddings\n",
    "\n",
    "Now we'll demonstrate how to index words,\n",
    "attach pre-trained word embeddings for them,\n",
    "and use such embeddings in Gluon.\n",
    "First, let's assign a unique ID and word vector to each word in the vocabulary\n",
    "in just a few lines of code.\n",
    "\n",
    "### Creating Vocabulary from Data Sets\n",
    "\n",
    "To begin, suppose that we have a simple text data set consisting of newline-separated strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" hello wor