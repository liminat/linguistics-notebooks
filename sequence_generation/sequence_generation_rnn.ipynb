{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Generation with Sampling and Beam Search\n",
    "\n",
    "This tutorial demonstrates how to sample sequences using a\n",
    "pre-trained language model in the following two ways:\n",
    "\n",
    "- with beam search\n",
    "sampler, and\n",
    "- with sequence sampler\n",
    "\n",
    "Let's use `V` to denote the vocabulary size, and `T` to denote the sequence\n",
    "length. Given a language model, we can sample sequences according to the\n",
    "probability that they would occur according to our model. At each time step, a\n",
    "language model predicts the likelihood of each word occurring, given the context\n",
    "from prior time steps. The outputs at any time step can be any word from the\n",
    "vocabulary whose size is V and thus the number of all possible outcomes for a\n",
    "sequence of length T is thus V^T.\n",
    "\n",
    "While sometimes we might want to sample\n",
    "sentences according to their probability of occurring, at other times we want to\n",
    "find the sentences that *are most likely to occur*. This is especially true in\n",
    "the case of language translation where we don't just want to see *a*\n",
    "translation. We want the *best* translation. While finding the optimal outcome\n",
    "quickly becomes intractable as time step increases, there are still many ways to\n",
    "sample reasonably good sequences. GluonNLP provides two samplers for generating\n",
    "from a language model: `BeamSearchSampler` and `SequenceSampler`.\n",
    "\n",
    "## Load Pretrained Language Model\n",
    "First, let's load a pretrained language model,\n",
    "from which we will sample sequences from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab file is not found. Downloading.\n",
      "Downloading /home/ec2-user/.mxnet/models/1562131339.294016wikitext-2-be36dc52.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/wikitext-2-be36dc52.zip...\n",
      "Downloading /home/ec2-user/.mxnet/models/awd_lstm_lm_1150_wikitext-2-f9562ed0.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/awd_lstm_lm_1150_wikitext-2-f9562ed0.zip...\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "ctx = mx.cpu()\n",
    "lm_model, vocab = nlp.model.get_model(name='awd_lstm_lm_1150',\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=True,\n",
    "                                      ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling a Sequence with `BeamSearchSampler`\n",
    "\n",
    "To overcome the exponential complexity in sequence decoding, beam search decodes\n",
    "greedily, keeping those sequences that are most likely based on the probability\n",
    "up to the current time step. The size of this subset is called the *beam size*.\n",
    "Suppose the beam size is `K` and the output vocabulary size is `V`. When\n",
    "selecting the beams to keep, the beam search algorithm first predict all\n",
    "possible successor words from the previous `K` beams, each of which has `V`\n",
    "possible outputs. This becomes a total of `K*V` paths. Out of these `K*V` paths,\n",
    "beam search ranks them by their score keeping only the top `K` paths.\n",
    "\n",
    "Let's take a look how to construct a `BeamSearchSampler`. The\n",
    "`nlp.model.BeamSearchSampler` class takes the following arguments for\n",
    "customization and extension:\n",
    "- beam_size : the beam size.\n",
    "- decoder : callable\n",
    "function of the one-step-ahead decoder.\n",
    "- eos_id : id of the EOS token.\n",
    "- scorer\n",
    ": the score function used in beam search.\n",
    "- max_length: the maximum search\n",
    "length.\n",
    "\n",
    "#### Scorer Function\n",
    "\n",
    "In this tutorial, we will use the `BeamSearchScorer` the\n",
    "as scorer, which implements the scoring function with length penalty in\n",
    "[Google NMT](https://arxiv.org/pdf/1609.08144.pdf) paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = nlp.model.BeamSearchScorer(alpha=0, K=5, from_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Function\n",
    "Next, we define the decoder based on the pretrained\n",
    "language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDecoder(object):\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "    def __call__(self, inputs, states):\n",
    "        outputs, states = self._model(mx.nd.expand_dims(inputs, axis=0), states)\n",
    "        return outputs[0], states\n",
    "    def state_info(self, *arg, **kwargs):\n",
    "        return self._model.state_info(*arg, **kwargs)\n",
    "decoder = LMDecoder(lm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Sampler\n",
    "\n",
    "Given a scorer and decoder, we are ready to create a sampler. We use symbol `.`\n",
    "to indicate the end of sentence (EOS). We can use vocab to get the index of the\n",
    "EOS, and then feed the index to the sampler. The following codes shows how to\n",
    "construct a beam search sampler. We will create a sampler with 4 beams and a\n",
    "maximum sample length of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_id = vocab['.']\n",
    "beam_sampler = nlp.model.BeamSearchSampler(beam_size=5,\n",
    "                                           decoder=decoder,\n",
    "                                           eos_id=eos_id,\n",
    "                                           scorer=scorer,\n",
    "                                           max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Sequences with Beam Search\n",
    "\n",
    "Next, we are going to generate sentences starting with \"I love it\" using beam\n",
    "search first. We feed ['I', 'Love'] to the language model to get the initial\n",
    "states and set the initial input to be the word 'it'. We will then print the\n",
    "top-3 generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = 'I love it'.split()\n",
    "bos_ids = [vocab[ele] for ele in bos]\n",
    "begin_states = lm_model.begin_state(batch_size=1, ctx=ctx)\n",
    "if len(bos_ids) > 1:\n",
    "    _, begin_states = lm_model(mx.nd.expand_dims(mx.nd.array(bos_ids[:-1]), axis=1),\n",
    "                               begin_states)\n",
    "inputs = mx.nd.full(shape=(1,), ctx=ctx, val=bos_ids[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(sampler, inputs, begin_states, num_print_outcomes):\n",
    "    samples, scores, valid_lengths = sampler(inputs, begin_states)\n",
    "    samples = samples[0].asnumpy()\n",
    "    scores = scores[0].asnumpy()\n",
    "    valid_lengths = valid_lengths[0].asnumpy()\n",
    "    print('Generation Result:')\n",
    "    for i in range(num_print_outcomes):\n",
    "        sentence = bos[:-1]\n",
    "        for ele in samples[i][:valid_lengths[i]]:\n",
    "            sentence.append(vocab.idx_to_token[ele])\n",
    "        print([' '.join(sentence), scores[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Result:\n",
      "['I love it .', -1.1080633]\n",
      "['I love it , and the <unk> of the <unk> .', -13.386149]\n",
      "['I love it , but it was not until the end of the year that it was not until the end of the .', -25.007332]\n",
      "['I love it , but it was not until the end of the 20th century that it was not until the end of .', -26.333738]\n",
      "['I love it , but it was not until the end of the 20th century that it was not until the early 1990s .', -28.628923]\n"
     ]
    }
   ],
   "source": [
    "generate_sequences(beam_sampler, inputs, begin_states, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling a Sequence with `SequenceSampler`\n",
    "\n",
    "The previous generation results\n",
    "may look a bit boring. Now, let's use sequence sampler to get some more\n",
    "interesting results.\n",
    "\n",
    "A `SequenceSampler` samples from the contextual multinomial distribution\n",
    "produced by the language model at each time step. Since we may want to control\n",
    "how \"sharp\" the distribution is to tradeoff diversity with correctness, we can\n",
    "use the temperature option in `SequenceSampler`, which controls the temperature\n",
    "of the softmax function.\n",
    "\n",
    "For each input same, sequence sampler can sample\n",
    "multiple **independent** sequences at once. The number of independent sequences\n",
    "to sample can be specified through the argument `beam_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_sampler = nlp.model.SequenceSampler(beam_size=5,\n",
    "                                        decoder=decoder,\n",
    "                                        eos_id=eos_id,\n",
    "                                        max_length=100,\n",
    "                                        temperature=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Sequences with Sequence Sampler\n",
    "Now, use the sequence sampler\n",
    "created to sample sequences based on the same inputs used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Result:\n",
      "['I love it and enjoy one of their series , Sanford Nöldeke , who sent his own attention to the series in 2002 .', -88.21921]\n",
      "['I love it in a television vein and rid him of his adventures .', -46.98534]\n",
      "['I love it for news .', -12.86888]\n",
      "['I love it ; it is not until the end of the year that a relative <unk> to a EEC owner is now raised .', -68.121025]\n",
      "['I love it in an 819 \" .', -21.320156]\n"
     ]
    }
   ],
   "source": [
    "generate_sequences(seq_sampler, inputs, begin_states, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "- Tweak alpha and K in BeamSearchScorer, how are the results\n",
    "changed?\n",
    "- Try different samples to decode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
