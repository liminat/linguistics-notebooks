{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for Intent Classification and Slot Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we will finetune a [BERT](https://arxiv.org/abs/1810.04805) model for the Intent Classification and Slot Labelling (ICSL) problem.\n",
    "\n",
    "Intent classification and slot labeling are two essential problems in Natural Language Understanding (NLU). In _intent classification_, the agent needs to detect the intention that the speaker's utterance conveys. For example, when the speaker says \"Book a flight from Long Beach to Seattle\", the intention is to book a flight ticket. In _slot labeling_, the agent needs to extract the semantic entities that are related to the intent. In our previous example, \"Long Beach\" and \"Seattle\" are two semantic constituents related to the flight, i.e., the origin and the destination.\n",
    "\n",
    "Essentially, _intent classification_ can be viewed as a sequence classification problem and _slot labeling_ can be viewed as a sequence tagging problem similar to Named-Entity Recognition (NER). Due to their inner correlation, these two tasks are usually trained jointly with a multi-task objective function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We first load the Airline Travel Information System (ATIS) dataset, which contains around 5000 utterance abouts travel plans and is a classical benchmark for ICSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the ATIS dataset\n",
      "#Train/Dev/Test = 4478/500/893\n",
      "#Intent         = 18\n",
      "#Slot           = 127\n"
     ]
    }
   ],
   "source": [
    "from gluonnlp.data import ATISDataset\n",
    "\n",
    "train_data = ATISDataset('train')\n",
    "dev_data = ATISDataset('dev')\n",
    "test_data = ATISDataset('test')\n",
    "intent_vocab = train_data.intent_vocab\n",
    "slot_vocab = train_data.slot_vocab\n",
    "print('Loaded the ATIS dataset')\n",
    "print('#Train/Dev/Test = {}/{}/{}'.format(len(train_data), len(dev_data), len(test_data)))\n",
    "print('#Intent         = {}'.format(len(intent_vocab)))\n",
    "print('#Slot           = {}'.format(len(slot_vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's display some samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: [\"i'm\", 'flying', 'from', 'boston', 'to', 'the', 'bay', 'area']\n",
      "    Tags: ['O', 'O', 'O', 'B-fromloc.city_name', 'O', 'O', 'B-toloc.city_name', 'I-toloc.city_name']\n",
      "   Label: atis_flight\n"
     ]
    }
   ],
   "source": [
    "print('Sentence:', dev_data[4][0])\n",
    "print('    Tags:', dev_data[4][1])\n",
    "print('   Label:', intent_vocab.idx_to_token[dev_data[4][2][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the BERT Model\n",
    "Next, we load the pretrained BERT model into the GPU. We load the BERT-base model trained on the Book Corpus + Wikipedia datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn, Block\n",
    "import gluonnlp as nlp\n",
    "import time\n",
    "import random\n",
    "from gluonnlp.data import BERTTokenizer\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)\n",
    "\n",
    "dropout_prob = 0.1\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "bert_model, bert_vocab = nlp.model.get_model(name='bert_12_768_12',\n",
    "                                             dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                             pretrained=True,\n",
    "                                             ctx=ctx,\n",
    "                                             use_pooler=True,\n",
    "                                             use_decoder=False,\n",
    "                                             use_classifier=False,\n",
    "                                             dropout=dropout_prob,\n",
    "                                             embed_dropout=dropout_prob)\n",
    "tokenizer = BERTTokenizer(bert_vocab, lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT uses the subword tokenization: e.g \"Sunnyvale\" will be tokenized as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sunny', '##vale']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Sunnyvale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we will convert the original ATIS dataset to make sure that the state corresponds to the first subword is used to predict the slot label.\n",
    "\n",
    "<img src=\"explain_subword_tagging.png\" width=\"480\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDSLSubwordTransform(object):\n",
    "    \"\"\"Transform the dataset using the bert vocabulary and tokenizer\n",
    "    \"\"\"\n",
    "    def __init__(self, subword_vocab, subword_tokenizer, slot_vocab, cased=False):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        subword_vocab : Vocab\n",
    "        subword_tokenizer : Tokenizer\n",
    "        cased : bool\n",
    "            Whether to convert all characters to lower\n",
    "        \"\"\"\n",
    "        super(IDSLSubwordTransform, self).__init__()\n",
    "        self._subword_vocab = subword_vocab\n",
    "        self._subword_tokenizer = subword_tokenizer\n",
    "        self._slot_vocab = slot_vocab\n",
    "        self._cased = cased\n",
    "        self._slot_pad_id = self._slot_vocab['O']\n",
    "\n",
    "\n",
    "    def __call__(self, word_tokens, tags, intent_ids):\n",
    "        \"\"\" Transform the dataset using the bert vocabulary and tokenizer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        word_tokens : List[str]\n",
    "        tags : List[str]\n",
    "        intent_ids : np.ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        subword_ids : np.ndarray\n",
    "        subword_mask : np.ndarray\n",
    "        selected : np.ndarray\n",
    "        padded_tag_ids : np.ndarray\n",
    "        intent_label : int\n",
    "        length : int\n",
    "        \"\"\"\n",
    "        subword_ids = []\n",
    "        subword_mask = []\n",
    "        selected = []\n",
    "        padded_tag_ids = []\n",
    "        intent_label = intent_ids[0]\n",
    "        ptr = 0\n",
    "        for token, tag in zip(word_tokens, tags):\n",
    "            if not self._cased:\n",
    "                token = token.lower()\n",
    "            token_sw_ids = self._subword_vocab[self._subword_tokenizer(token)]\n",
    "            subword_ids.extend(token_sw_ids)\n",
    "            subword_mask.extend([1] + [0] * (len(token_sw_ids) - 1))\n",
    "            selected.append(ptr)\n",
    "            padded_tag_ids.extend([self._slot_vocab[tag]] +\n",
    "                                  [self._slot_pad_id] * (len(token_sw_ids) - 1))\n",
    "            ptr += len(token_sw_ids)\n",
    "        length = len(subword_ids)\n",
    "        if len(subword_ids) != len(padded_tag_ids):\n",
    "            print(word_tokens)\n",
    "            print(tags)\n",
    "            print(subword_ids)\n",
    "            print(padded_tag_ids)\n",
    "        return np.array(subword_ids, dtype=np.int32),\\\n",
    "               np.array(subword_mask, dtype=np.int32),\\\n",
    "               np.array(selected, dtype=np.int32),\\\n",
    "               np.array(padded_tag_ids, dtype=np.int32),\\\n",
    "               intent_label,\\\n",
    "               length\n",
    "\n",
    "idsl_transform = IDSLSubwordTransform(subword_vocab=bert_vocab,\n",
    "                                      subword_tokenizer=tokenizer,\n",
    "                                      slot_vocab=slot_vocab,\n",
    "                                      cased=False)\n",
    "train_data_bert = train_data.transform(idsl_transform, lazy=False)\n",
    "dev_data_bert = dev_data.transform(idsl_transform, lazy=False)\n",
    "test_data_bert = test_data.transform(idsl_transform, lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sequence: ['what', 'are', 'the', 'cheapest', 'round', 'trip', 'flights', 'from', 'denver', 'to', 'atlanta']\n",
      "subword sequence: ['what', 'are', 'the', 'cheap', '##est', 'round', 'trip', 'flights', 'from', 'denver', 'to', 'atlanta']\n",
      "mask: [1 1 1 1 0 1 1 1 1 1 1 1]\n",
      "index of the first subword: [ 0  1  2  3  5  6  7  8  9 10 11]\n",
      "slot label: [126 126 126  21 126  66 117 126 126  48 126  78]\n",
      "intent label: 10\n",
      "length: 12\n"
     ]
    }
   ],
   "source": [
    "print('original sequence:', dev_data[26][0])\n",
    "print('subword sequence:', [bert_vocab.idx_to_token[ele] for ele in dev_data_bert[26][0]])\n",
    "print('mask:', dev_data_bert[26][1])\n",
    "print('index of the first subword:', dev_data_bert[26][2])\n",
    "print('slot label:', dev_data_bert[26][3])\n",
    "print('intent label:'