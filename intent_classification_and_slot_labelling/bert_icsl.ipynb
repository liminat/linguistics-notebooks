{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for Intent Classification and Slot Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we will finetune a [BERT](https://arxiv.org/abs/1810.04805) model for the Intent Classification and Slot Labelling (ICSL) problem.\n",
    "\n",
    "Intent classification and slot labeling are two essential problems in Natural Language Understanding (NLU). In _intent classification_, the agent needs to detect the intention that the speaker's utterance conveys. For example, when the speaker says \"Book a flight from Long Beach to Seattle\", the intention is to book a flight ticket. In _slot labeling_, the agent needs to extract the semantic entities that are related to the intent. In our previous example, \"Long Beach\" and \"Seattle\" are two semantic constituents related to the flight, i.e., the origin and the destination.\n",
    "\n",
    "Essentially, _intent classification_ can be viewed as a sequence classification problem and _slot labeling_ can be viewed as a sequence tagging problem similar to Named-Entity Recognition (NER). Due to their inner correlation, these two tasks are usually trained jointly with a multi-task objective function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We first load the Airline Travel Information System (ATIS) dataset, which contains around 5000 utterance abouts travel plans and is a classical benchmark for ICSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the ATIS dataset\n",
      "#Train/Dev/Test = 4478/500/893\n",
      "#Intent         = 18\n",
      "#Slot           = 127\n"
     ]
    }
   ],
   "source": [
    "from gluonnlp.data import ATISDataset\n",
    "\n",
    "train_data = ATISDataset('train')\n",
    "dev_data = ATISDataset('dev')\n",
    "test_data = ATISDataset('test')\n",
    "intent_vocab = train_data.intent_vocab\n",
    "slot_vocab = train_data.slot_vocab\n",
    "print('Loaded the ATIS dataset')\n",
    "print('#Train/Dev/Test = {}/{}/{}'.format(len(train_data), len(dev_data), len(test_data)))\n",
    "print('#Intent         = {}'.format(len(intent_vocab)))\n",
    "print('#Slot           = {}'.format(len(slot_vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's display some samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: [\"i'm\", 'flying', 'from', 'boston', 'to', 'the', 'bay', 'area']\n",
      "    Tags: ['O', 'O', 'O', 'B-fromloc.city_name', 'O', 'O', 'B-toloc.city_name', 'I-toloc.city_name']\n",
      "   Label: atis_flight\n"
     ]
    }
   ],
   "source": [
    "print('Sentence:', dev_data[4][0])\n",
    "print('    Tags:', dev_data[4][1])\n",
    "print('   Label:', intent_vocab.idx_to_token[dev_data[4][2][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the BERT Model\n",
    "Next, we load the pretrained BERT model into the GPU. We load the BERT-base model trained on the Book Corpus + Wikipedia datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn, Block\n",
    "import gluonnlp as nlp\n",
    "import time\n",
    "import random\n",
    "from gluonnlp.data import BERTTokenizer\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)\n",
    "\n",
    "dropout_prob = 0.1\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "bert_model, bert_vocab = nlp.model.get_model(name='bert_12_768_12',\n",
    "                                             dataset_name='b