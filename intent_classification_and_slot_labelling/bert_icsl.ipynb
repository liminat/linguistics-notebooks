{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for Intent Classification and Slot Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we will finetune a [BERT](https://arxiv.org/abs/1810.04805) model for the Intent Classification and Slot Labelling (ICSL) problem.\n",
    "\n",
    "Intent classification and slot labeling are two essential problems in Natural Language Understanding (NLU). In _intent classification_, the agent needs to detect the intention that the speaker's utterance conveys. For example, when the speaker says \"Book a flight from Long Beach to Seattle\", the intention is to book a flight ticket. In _slot labeling_, the agent needs to extract the semantic entities that are related to the intent. In our previous example, \"Long Beach\" and \"Seattle\" are two semantic constituents related to the flight, i.e., the origin and the destination.\n",
    "\n",
    "Essentially, _intent classification_ can be viewed as a sequence classification problem and _slot labeling_ can be viewed as a sequence tagging problem similar to Named-Entity Recognition (NER). Due to their inner correlation, these two tasks are usually trained jointly with a multi-task objective function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We first load the Airline Travel Information System (ATIS) dataset, which contains around 5000 utterance abouts travel plans and is a classical benchmark for ICSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the ATIS dataset\n",
      "#Train/Dev/Test = 4478/500/893\n",
      "#Intent         = 18\n",
      "#Slot           = 127\n"
     ]
    }
   ],
   "source": [
    "from gluonnlp.data import ATISDataset\n",
    "\n",
    "train_data = ATISDataset('train')\n",
    "dev_data = ATISDataset('dev')\n",
    "test_data = ATISDataset('test')\n",
    "intent_vocab = train_data.intent_vocab\n",
    "slot_vocab = train_data.slot_vocab\n",
    "print('Loaded the ATIS dataset')\n",
    "print('#Train/Dev/Test = {}/{}/{}'.format(len(train_data), len(dev_data), len(test_data)))\n",
    "print('#Intent         = {}'.format(len(intent_vocab)))\n",
    "print('#Slot           = {}'.format(len(slot_vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's display some samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: [\"i'm\", 'flying', 'from', 'boston', 'to', 'the', 'bay', 'area']\n",
      "    Tags: ['O', 'O', 'O', 'B-fromloc.city_name', 'O', 'O', 'B-toloc.city_name', 'I-toloc.city_name']\n",
      "   Label: atis_flight\n"
     ]
    }
   ],
   "source": [
    "print('Sentence:', dev_data[4][0])\n",
    "print('    Tags:', dev_data[4][1])\n",
    "print('   Label:', intent_vocab.idx_to_token[dev_data[4][2][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the BERT Model\n",
    "Next, we load the pretrained BERT model into the GPU. We load the BERT-base model trained on the Book Corpus + Wikipedia datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn, Block\n",
    "import gluonnlp as nlp\n",
    "import time\n",
    "import random\n",
    "from gluonnlp.data import BERTTokenizer\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)\n",
    "\n",
    "dropout_prob = 0.1\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "bert_model, bert_vocab = nlp.model.get_model(name='bert_12_768_12',\n",
    "                                             dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                             pretrained=True,\n",
    "                                             ctx=ctx,\n",
    "                                             use_pooler=True,\n",
    "                                             use_decoder=False,\n",
    "                                             use_classifier=False,\n",
    "                                             dropout=dropout_prob,\n",
    "                                             embed_dropout=dropout_prob)\n",
    "tokenizer = BERTTokenizer(bert_vocab, lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT uses the subword tokenization: e.g \"Sunnyvale\" will be tokenized as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sunny', '##vale']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Sunnyvale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we will convert the original ATIS dataset to make sure that the state corresponds to the first subword is used to predict the slot label.\n",
    "\n",
    "<img src=\"explain_subword_tagging.png\" width=\"480\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDSLSubwordTransform(object):\n",
    "    \"\"\"Transform the dataset using the bert vocabulary and tokenizer\n",
    "    \"\"\"\n",
    "    def __init__(self, subword_vocab, subword_tokenizer, slot_vocab, cased=False):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        subword_vocab : Vocab\n",
    "        subword_tokenizer : Tokenizer\n",
    "        cased : bool\n",
    "            Whether to convert all characters to lower\n",
    "        \"\"\"\n",
    "        super(IDSLSubwordTransform, self).__init__()\n",
    "        self._subword_vocab = subword_vocab\n",
    "        self._subword_tokenizer = subword_tokenizer\n",
    "        self._slot_vocab = slot_vocab\n",
    "        self._cased = cased\n",
    "        self._slot_pad_id = self._slot_vocab['O']\n",
    "\n",
    "\n",
    "    def __call__(self, word_tokens, tags, intent_ids):\n",
    "        \"\"\" Transform the dataset using the bert vocabulary and tokenizer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        word_tokens : List[str]\n",
    "        tags : List[str]\n",
    "        intent_ids : np.ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        subword_ids : np.ndarray\n",
    "        subword_mask : np.ndarray\n",
    "        selected : np.ndarray\n",
    "        padded_tag_ids : np.ndarray\n",
    "        intent_label : int\n",
    "        length : int\n",
    "        \"\"\"\n",
    "        subword_ids = []\n",
    "        subword_mask = []\n",
    "        selected = []\n",
    "        padded_tag_ids = []\n",
    "        intent_label = intent_ids[0]\n",
    "        ptr = 0\n",
    "        for token, tag in zip(word_tokens, tags):\n",
    "            if not self._cased:\n",
    "                token = token.lower()\n",
    "            token_sw_ids = self._subword_vocab[self._subword_tokenizer(token)]\n",
    "            subword_ids.extend(token_sw_ids)\n",
    "            subword_mask.extend([1] + [0] * (len(token_sw_ids) - 1))\n",
    "            selected.append(ptr)\n",
    "            padded_tag_ids.extend([self._slot_vocab[tag]] +\n",
    "                                  [self._slot_pad_id] * (len(token_sw_ids) - 1))\n",
    "            ptr += len(token_sw_ids)\n",
    "        length = len(subword_ids)\n",
    "        if len(subword_ids) != len(padded_tag_ids):\n",
    "            print(word_tokens)\n",
    "            print(tags)\n",
    "            print(subword_ids)\n",
    "            print(padded_tag_ids)\n",
    "        return np.array(subword_ids, dtype=np.int32),\\\n",
    "               np.array(subword_mask, dtype=np.int32),\\\n",
    "               np.array(selected, dtype=np.int32),\\\n",
    "               np.array(padded_tag_ids, dtype=np.int32),\\\n",
    "               intent_label,\\\n",
    "               length\n",
    "\n",
    "idsl_transform = IDSLSubwordTransform(subword_vocab=bert_vocab,\n",
    "                                      subword_tokenizer=tokenizer,\n",
    "                                      slot_vocab=slot_vocab,\n",
    "                                      cased=False)\n",
    "train_data_bert = train_data.transform(idsl_transform, lazy=False)\n",
    "dev_data_bert = dev_data.transform(idsl_transform, lazy=False)\n",
    "test_data_bert = test_data.transform(idsl_transform, lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sequence: ['what', 'are', 'the', 'cheapest', 'round', 'trip', 'flights', 'from', 'denver', 'to', 'atlanta']\n",
      "subword sequence: ['what', 'are', 'the', 'cheap', '##est', 'round', 'trip', 'flights', 'from', 'denver', 'to', 'atlanta']\n",
      "mask: [1 1 1 1 0 1 1 1 1 1 1 1]\n",
      "index of the first subword: [ 0  1  2  3  5  6  7  8  9 10 11]\n",
      "slot label: [126 126 126  21 126  66 117 126 126  48 126  78]\n",
      "intent label: 10\n",
      "length: 12\n"
     ]
    }
   ],
   "source": [
    "print('original sequence:', dev_data[26][0])\n",
    "print('subword sequence:', [bert_vocab.idx_to_token[ele] for ele in dev_data_bert[26][0]])\n",
    "print('mask:', dev_data_bert[26][1])\n",
    "print('index of the first subword:', dev_data_bert[26][2])\n",
    "print('slot label:', dev_data_bert[26][3])\n",
    "print('intent label:', dev_data_bert[26][4])\n",
    "print('length:', dev_data_bert[26][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Training Network\n",
    "We add two fully-connected layers on top of BERT to predict the slot labels and intent labels, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForICSL(Block):\n",
    "    def __init__(self, bert, num_intent_classes, num_slot_classes, dropout_prob,\n",
    "                 prefix=None, params=None):\n",
    "        \"\"\" Initialize the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bert : Block\n",
    "        num_intent_classes : int\n",
    "        num_slot_classes : int\n",
    "        dropout_prob : float\n",
    "        prefix : None or str\n",
    "        params : None or ParamDict\n",
    "        \"\"\"\n",
    "        super(BERTForICSL, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.intent_classifier = nn.HybridSequential()\n",
    "            with self.intent_classifier.name_scope():\n",
    "                self.intent_classifier.add(nn.Dropout(rate=dropout_prob))\n",
    "                self.intent_classifier.add(nn.Dense(units=num_intent_classes, flatten=False))\n",
    "            self.slot_tagger = nn.HybridSequential()\n",
    "            with self.slot_tagger.name_scope():\n",
    "                self.slot_tagger.add(nn.Dropout(rate=dropout_prob))\n",
    "                self.slot_tagger.add(nn.Dense(units=num_slot_classes, flatten=False))\n",
    "\n",
    "    def forward(self, inputs, valid_length):\n",
    "        \"\"\" Forward\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : NDArray\n",
    "            The input sentences, has shape (batch_size, seq_length)\n",
    "        valid_length : NDArray\n",
    "            The valid length of the sentences\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        intent_scores : NDArray\n",
    "            Shape (batch_size, num_classes)\n",
    "        slot_scores : NDArray\n",
    "            Shape (batch_size, seq_length, num_tag_types)\n",
    "        \"\"\"\n",
    "        token_types = mx.nd.zeros_like(inputs)\n",
    "        encoded_states, pooler_out = self.bert(inputs, token_types, valid_length)\n",
    "        intent_scores = self.intent_classifier(pooler_out)\n",
    "        slot_scores = self.slot_tagger(encoded_states)\n",
    "        return intent_scores, slot_scores\n",
    "net = BERTForICSL(bert_model, num_intent_classes=len(intent_vocab),\n",
    "                  num_slot_classes=len(slot_vocab), dropout_prob=dropout_prob)\n",
    "net.slot_tagger.initialize(ctx=ctx, init=mx.init.Normal(0.02))\n",
    "net.intent_classifier.initialize(ctx=ctx, init=mx.init.Normal(0.02))\n",
    "net.hybridize()\n",
    "intent_pred_loss = gluon.loss.SoftmaxCELoss()\n",
    "slot_pred_loss = gluon.loss.SoftmaxCELoss(batch_axis=[0, 1])\n",
    "intent_pred_loss.hybridize()\n",
    "slot_pred_loss.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTForICSL(\n",
      "  (intent_classifier): HybridSequential(\n",
      "    (0): Dropout(p = 0.1, axes=())\n",
      "    (1): Dense(None -> 18, linear)\n",
      "  )\n",
      "  (bert): BERTModel(\n",
      "    (pooler): Dense(768 -> 768, Activation(tanh))\n",
      "    (token_type_embed): HybridSequential(\n",
      "      (0): Embedding(2 -> 768, float32)\n",
      "      (1): Dropout(p = 0.1, axes=())\n",
      "    )\n",
      "    (word_embed): HybridSequential(\n",
      "      (0): Embedding(30522 -> 768, float32)\n",
      "      (1): Dropout(p = 0.1, axes=())\n",
      "    )\n",
      "    (encoder): BERTEncoder(\n",
      "      (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "      (transformer_cells): HybridSequential(\n",
      "        (0): BERTEncoderCell(\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "            (activation): GELU()\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "          )\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): BERTEncoderCell(\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "            (activation): GELU()\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "          )\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): BERTEncoderCell(\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "            (activation): GELU()\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "          )\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): BERTEncoderCell(\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "            (activation): GELU()\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "          )\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): BERTEncoderCell(\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "            (activation): GELU()\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "          )\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): BERTEncoderCell(\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "            (activation): GELU()\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "          )\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (6): BERTEncoderCell(\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "            (activation): GELU()\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "          )\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (7): BERTEncoderCell(\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "            (activation): GELU()\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "          )\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (8): BERTEncoderCell(\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "            (activation): GELU()\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "          )\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (9): BERTEncoderCell(\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "            (activation): GELU()\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "          )\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (10): BERTEncoderCell(\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "            (activation): GELU()\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "          )\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (11): BERTEncoderCell(\n",
      "          (proj): Dense(768 -> 768, linear)\n",
      "          (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (ffn): BERTPositionwiseFFN(\n",
      "            (ffn_1): Dense(768 -> 3072, linear)\n",
      "            (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "            (activation): GELU()\n",
      "            (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            (ffn_2): Dense(3072 -> 768, linear)\n",
      "          )\n",
      "          (attention_cell): MultiHeadAttentionCell(\n",
      "            (proj_query): Dense(768 -> 768, linear)\n",
      "            (proj_key): Dense(768 -> 768, linear)\n",
      "            (proj_value): Dense(768 -> 768, linear)\n",
      "            (_base_cell): DotProductAttentionCell(\n",
      "              (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): BERTLayerNorm(scale=True, eps=1e-12, center=True, axis=-1, in_channels=768)\n",
      "    )\n",
      "  )\n",
      "  (slot_tagger): HybridSequential(\n",
      "    (0): Dropout(p = 0.1, axes=())\n",
      "    (1): Dense(None -> 127, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the DataLoader and Trainer for Training/Validation/Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 5E-5\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'bertadam',\n",
    "                        {'learning_rate': learning_rate, 'wd': 0.0})\n",
    "batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(pad_val=0),    # Subword ID\n",
    "                                      nlp.data.batchify.Pad(pad_val=0),    # Subword Mask\n",
    "                                      nlp.data.batchify.Pad(pad_val=0),    # Beginning of subword\n",
    "                                      nlp.data.batchify.Pad(pad_val=0),    # Tag IDs\n",
    "                                      nlp.data.batchify.Stack(),           # Intent Label\n",
    "                                      nlp.data.batchify.Stack())           # Valid Length\n",
    "train_batch_sampler = nlp.data.sampler.SortedBucketSampler(\n",
    "    [len(ele) for ele in train_data_bert],\n",
    "    batch_size=batch_size,\n",
    "    mult=20,\n",
    "    shuffle=True)\n",
    "train_loader = gluon.data.DataLoader(dataset=train_data_bert,\n",
    "                                     num_workers=4,\n",
    "                                     batch_sampler=train_batch_sampler,\n",
    "                                     batchify_fn=batchify_fn)\n",
    "dev_loader = gluon.data.DataLoader(dataset=dev_data_bert,\n",
    "                                   num_workers=4,\n",
    "                                   batch_size=batch_size,\n",
    "                                   batchify_fn=batchify_fn,\n",
    "                                   shuffle=False)\n",
    "test_loader = gluon.data.DataLoader(dataset=test_data_bert,\n",
    "                                    num_workers=4,\n",
    "                                    batch_size=batch_size,\n",
    "                                    batchify_fn=batchify_fn,\n",
    "                                    shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [00:17<00:00, 19.75it/s]\n",
      "[Epoch 0] train intent/slot = 1.155/1.854, #token per second=3012\n",
      "100%|██████████| 280/280 [00:15<00:00, 19.02it/s]\n",
      "[Epoch 1] train intent/slot = 0.311/0.507, #token per second=3472\n",
      "100%|██████████| 280/280 [00:15<00:00, 18.42it/s]\n",
      "[Epoch 2] train intent/slot = 0.144/0.240, #token per second=3436\n",
      "100%|██████████| 280/280 [00:15<00:00, 18.25it/s]\n",
      "[Epoch 3] train intent/slot = 0.098/0.147, #token per second=3427\n",
      "100%|██████████| 280/280 [00:15<00:00, 16.67it/s]\n",
      "[Epoch 4] train intent/slot = 0.068/0.095, #token per second=3452\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "nepochs = 40\n",
    "train_epochs = 5  # IMPORTANT!! We break after 5 epochs due to the time limit. \n",
    "                  # You may remove this variable if you want to gain the best performance\n",
    "warmup_ratio = 0.1\n",
    "step_num = 0\n",
    "num_train_steps = int(len(train_batch_sampler) * nepochs)\n",
    "num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
    "best_dev_sf1 = -1\n",
    "for epoch_id in range(nepochs):\n",
    "    if epoch_id >= train_epochs:\n",
    "        break\n",
    "    avg_train_intent_loss = 0.0\n",
    "    avg_train_slot_loss = 0.0\n",
    "    nsample = 0\n",
    "    nslot = 0\n",
    "    ntoken = 0\n",
    "    train_epoch_start = time.time()\n",
    "    for token_ids, mask, selected, slot_ids, intent_label, valid_length in tqdm(train_loader, file=sys.stdout):\n",
    "        # Copy data to the context, i.e., GPU in our example\n",
    "        token_ids = mx.nd.array(token_ids, ctx=ctx).astype(np.int32)\n",
    "        mask = mx.nd.array(mask, ctx=ctx).astype(np.float32)\n",
    "        slot_ids = mx.nd.array(slot_ids, ctx=ctx).astype(np.int32)\n",
    "        intent_label = mx.nd.array(intent_label, ctx=ctx).astype(np.int32)\n",
    "        valid_length = mx.nd.array(valid_length, ctx=ctx).astype(np.float32)\n",
    "        batch_nslots = mask.sum().asscalar()\n",
    "        batch_nsample = token_ids.shape[0]\n",
    "\n",
    "        # Set learning rate warm-up\n",
    "        step_num += 1\n",
    "        if step_num < num_warmup_steps:\n",
    "            new_lr = learning_rate * step_num / num_warmup_steps\n",
    "        else:\n",
    "            offset = ((step_num - num_warmup_steps) * learning_rate /\n",
    "                      (num_train_steps - num_warmup_steps))\n",
    "            new_lr = learning_rate - offset\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "\n",
    "        # Begin to calculate the gradient\n",
    "        with 